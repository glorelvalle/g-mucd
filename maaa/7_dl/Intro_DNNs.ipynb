{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"title\">Introduction to Deep Neural Networks</div>\n",
    "<div class=\"subtitle\">Métodos Avanzados en Aprendizaje Automático</div>\n",
    "<div class=\"author\">Carlos María Alaíz Gudín - Universidad Autónoma de Madrid</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initial Configuration**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell defines the configuration of Jupyter Notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<head><link rel=\"stylesheet\" href=\"style.css\"></head>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<head><link rel=\"stylesheet\" href=\"style.css\"></head>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell imports the packages to be used (all of them quite standard except for `Utils`, which is provided with the notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_moons, load_sample_images, load_digits\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "from tensorflow.python.util import deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import logging\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "from Utils import plot_dataset, plot_dataset_clas, plot_nonlinear_model_clas\n",
    "\n",
    "matplotlib.rc('figure', figsize=(15, 5))\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "seed = 123\n",
    "\n",
    "gpu_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "for device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanishing Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example illustrates the phenomenon of the vanishing gradient over a simple synthetic example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of the MSE over a single pattern will be computed.\n",
    "In particular, both the input $\\mathbf{x}$ and the output $\\mathbf{y}$ belong to $\\mathbb{R}^5$.\n",
    "Moreover, all the layers have $5$ hidden units, so the dimension of the weights between every pair of layer is a $5 \\times 5$ matrix.\n",
    "No bias are considered\n",
    "The sigmoid activation function is used.\n",
    "\n",
    "The weights are randomly intialized following a Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dim = 5\n",
    "n_lay = 50\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Pattern.\n",
    "x = np.random.randn(n_dim)\n",
    "y = np.random.randn(n_dim)\n",
    "\n",
    "# Activation function and its derivative.\n",
    "g = lambda x : 1 / (1 + np.exp(- x))\n",
    "gp = lambda x : g(x) * (1 - g(x))\n",
    "\n",
    "# Weights.\n",
    "w = np.random.randn(n_lay, n_dim, n_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell does the forward pass, computing the local field and output of each unit for the input pattern $\\mathbf{x}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = np.zeros((n_dim, n_lay + 1))\n",
    "a = np.zeros((n_dim, n_lay + 1))\n",
    "\n",
    "o[:, 0] = x\n",
    "for l in range(1, n_lay + 1):\n",
    "    a[:, l] = w[l - 1, :, :] @ o[:, l - 1]\n",
    "    o[:, l] = g(a[:, l])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Does the previous code coincide with the algorithm studied in theory?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell does the backward pass, computing first the error deltas, and then the partial derivative with respect to each weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.zeros((n_dim, n_lay + 1))\n",
    "\n",
    "d[:, -1] = (o[:, -1] - y) * gp(a[:, -1])\n",
    "for l in range(n_lay, 0, -1):\n",
    "    d[:, l - 1] = (w[l - 1, :, :] @ d[:, l]) * gp(a[:, l])\n",
    "\n",
    "grad = np.zeros_like(w)\n",
    "for l in range(n_lay):\n",
    "    grad[l, :, :] = d[:, l, None] * o[:, None, l]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Does the previous code coincide with the algorithm studied in theory?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell computes the norm of the gradient at each layer of the network, depicting it in logarithmic scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norms = np.zeros(n_lay)\n",
    "for l in range(n_lay):\n",
    "    norms[l] = np.linalg.norm(grad[l])\n",
    "\n",
    "plt.semilogy(range(1, n_lay + 1), norms)\n",
    "plt.grid()\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Gradient Norm')\n",
    "plt.gca().invert_xaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* According to the previous figure, how much slower is the training of the $30$-th layer than the training of the output layer?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation of NNs of `sklearn` initializes the weights using the following uniform distribution:\n",
    "$$ w^{(\\ell)}_{ij} \\sim  \\mathcal{U} \\left ( - \\sqrt{\\frac{6}{d_{\\ell + 1} + d_\\ell}}, + \\sqrt{\\frac{6}{d_{\\ell + 1} + d_\\ell}} \\right ) . $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below creates a NN and trains it over a random dataset during only one iteration with an almost zero learning rate, so that the final weights are practically equal to the initial ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pat = 10\n",
    "n_dim = 5\n",
    "layers = (20, 500, 100)\n",
    "\n",
    "x = np.zeros((n_pat, n_dim))\n",
    "y = np.zeros(n_pat)\n",
    "\n",
    "model = MLPClassifier(hidden_layer_sizes=layers, max_iter=1, learning_rate_init=1e-50, random_state=seed)\n",
    "model = model.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of the Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell shows the distribution of the final weights for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in range(len(layers) + 1):\n",
    "    bound = np.sqrt(6. / (model.coefs_[l].shape[0] + model.coefs_[l].shape[1]))\n",
    "\n",
    "    plt.hist(model.coefs_[l].ravel(), density=True, alpha=0.5)\n",
    "    plt.scatter([-bound, bound], [0, 0], marker='|', s=2000, c='k')\n",
    "\n",
    "    plt.title('Distribution of $\\mathbf{W}^{(%d)}$' % l)\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Density')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Does the resultant empirical distribution match the theoretical one?\n",
    "\n",
    "*Note*: The vertical black marks indicate the bounds of the uniform distribution defined above.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illustration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code depicts the activation functions available in `sklearn`, by building a NN with a single hidden unit and a linear output, so that the only transformation is precisely the activation function of the hidden unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pat = 100\n",
    "x = np.linspace(-3, 3, n_pat).reshape(-1, 1)\n",
    "y = np.zeros(n_pat)\n",
    "\n",
    "for activation in ('identity', 'logistic', 'tanh', 'relu'):\n",
    "    model = MLPRegressor(hidden_layer_sizes=(1,), activation=activation, max_iter=1, random_state=seed)\n",
    "    model.fit(x, y)\n",
    "    model.coefs_ = [np.array([[1]]), np.array([[1]])]\n",
    "    model.intercepts_ = [np.array([0]), np.array([0])]\n",
    "\n",
    "    plt.plot(x.ravel(), model.predict(x), label=activation)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Can you distinguish the different activation functions?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below generates the 2-dimensional non-linear classification dataset of the two moons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pat = 200\n",
    "noise = 0.15\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "x, y = make_moons(n_samples=n_pat, noise=noise, random_state=seed)\n",
    "y[y == 0] = -1\n",
    "plot_dataset_clas(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell trains four NNs with two hidden layers and $100$ hidden units in each layer, using the different activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for activation in ('identity', 'logistic', 'tanh', 'relu'):\n",
    "    model = MLPClassifier(hidden_layer_sizes=(100,100), activation=activation, max_iter=1000, random_state=seed)\n",
    "    model.fit(x, y)\n",
    "\n",
    "    plot_nonlinear_model_clas(x, y, model, phi=lambda x: x)\n",
    "    print('Activation: ' + activation)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Which activation function works best?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell loads a sample image from `sklearn` and shows it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "china = load_sample_images().images[0]\n",
    "\n",
    "plt.imshow(china)\n",
    "plt.title('Original')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Perturbations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random perturbations of a set of images can be generated using `keras`.\n",
    "This is done using the previous sample image in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = keras.preprocessing.image.ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "x = keras.preprocessing.image.img_to_array(china)\n",
    "x = x.reshape((1,) + x.shape)\n",
    "\n",
    "n_rows = 4\n",
    "n_cols = 4\n",
    "n_images = n_rows * n_cols\n",
    "\n",
    "i = 0\n",
    "plt.figure(figsize=(15, 15))\n",
    "for batch in datagen.flow(x, batch_size=1):\n",
    "    i += 1\n",
    "    if i > n_images:\n",
    "        break\n",
    "\n",
    "    plt.subplot(n_rows, n_cols, i)\n",
    "    plt.imshow(keras.preprocessing.image.array_to_img(batch[0]))\n",
    "    plt.title('Sample %d' % i)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Can you distinguish the different distorting effects?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell loads the simplified version of the MNIST dataset available in `sklearn`.\n",
    "The dataset is modified to increase the size of the images to $80 \\times 80$ (repeating the same pixel $10$ times along each axis) and adding colours (repeating the same value in the $3$ channels), so that it is compatible with sophisticated NN architectures.\n",
    "\n",
    "After that, the dataset is divided into train and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_orig, y = load_digits(n_class=10, return_X_y=True)\n",
    "\n",
    "x = []\n",
    "for img in x_orig:\n",
    "    img = img.reshape(8, 8).repeat(10, axis=0).repeat(10, axis=1)[:,:,None].repeat(3, axis=2).reshape(80, 80, 3)\n",
    "    img = img / 8 - 1\n",
    "    x.append(img)\n",
    "x_tr, x_va, y_tr, y_va = train_test_split(np.array(x), y, test_size=0.6, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell creates a model based on a pretrained NN, adding a trainable dense layer at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = keras.applications.Xception(\n",
    "    weights=\"imagenet\",\n",
    "    input_shape=(80, 80, 3),\n",
    "    include_top=False,\n",
    ")\n",
    "\n",
    "base_model.trainable = False\n",
    "inputs = keras.Input(shape=(80, 80, 3))\n",
    "layers = base_model(inputs, training=False)\n",
    "layers = keras.layers.GlobalAveragePooling2D()(layers)\n",
    "layers = keras.layers.Dropout(0.2)(layers)\n",
    "outputs = keras.layers.Dense(10, activation='softmax')(layers)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* What percentage of the NN parameters will be modified?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can be now trained, while keeping frozen the pretrained layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "epochs = 5\n",
    "history = model.fit(x_tr, y_tr, epochs=epochs, validation_data=(x_va, y_va))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* How does the model perform?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do the fine tuning, all the parameters are set as trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_model.trainable = True\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the model can be now trained, using a smaller learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-5),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "epochs = 2\n",
    "history = model.fit(x_tr, y_tr, epochs=epochs, validation_data=(x_va, y_va))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* How does the model perform?\n",
    "* Did the fine tuning improve the results?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code generates a simple regression problem using a parabola with noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "\n",
    "n_pat = 64\n",
    "lim = 3\n",
    "noise = 1e0\n",
    "x_orig = np.linspace(- lim, lim, n_pat)\n",
    "x_long = np.linspace(- lim, lim, 10 * n_pat)\n",
    "y = np.square(x_orig) + noise * np.random.randn(n_pat)\n",
    "x = x_orig.reshape(-1, 1)\n",
    "\n",
    "plot_dataset(x_orig, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different NNs with two hidden layers and a dropout intermediate layer are trained and used to predict over this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in (0.0, 0.2, 0.4, 0.6, 0.8):\n",
    "    inputs = keras.Input(shape=(1))\n",
    "    layers = keras.layers.Dense(100, activation=\"relu\")(inputs)\n",
    "    layers = keras.layers.Dropout(d)(layers)\n",
    "    layers = keras.layers.Dense(100, activation=\"relu\")(layers)\n",
    "    outputs = keras.layers.Dense(1)(layers)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        loss=keras.losses.MeanSquaredError(),\n",
    "        metrics=[\"mean_squared_error\"],\n",
    "    )\n",
    "\n",
    "    epochs = 1000\n",
    "    print(\"Fitting NN with dropout at %.0f%%\" % (100 * d))\n",
    "    model.fit(x, y, epochs=epochs, verbose=0)\n",
    "    plt.plot(x_long, model.predict(x_long.reshape(-1, 1)), label=\"Dropout %.0f%%\" % (100 * d))\n",
    "\n",
    "plt.plot(x_orig, y, \"*k\", label=\"Observed\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* What is the effect of the dropout?\n",
    "* Is it helping to prevent over-fitting?\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "197px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
