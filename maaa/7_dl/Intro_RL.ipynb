{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"title\">Introduction to Reinforcement Learning</div>\n",
    "<div class=\"subtitle\">Métodos Avanzados en Aprendizaje Automático</div>\n",
    "<div class=\"author\">Carlos María Alaíz Gudín - Universidad Autónoma de Madrid</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initial Configuration**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell defines the configuration of Jupyter Notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<head><link rel=\"stylesheet\" href=\"style.css\"></head>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<head><link rel=\"stylesheet\" href=\"style.css\"></head>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell imports the packages to be used (all of them quite standard except for `Utils`, which is provided with the notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "import tensorflow as tf\n",
    "gpu_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "for device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "from tensorflow import keras\n",
    "from IPython import display\n",
    "\n",
    "from Utils import generate_sequence\n",
    "\n",
    "seed = 123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alongside with **Supervised Learning** and **Unsupervised Learning**, there is another machine learning paradigm called **Reinforcement Learning** (RL), which tries to determine how an agent ought to take actions in an environment in order to maximize a cumulative reward.\n",
    "\n",
    "This type of problems is usually formalized as a Markov Decision Process:\n",
    "* $s_t$ is the state at time $t$. Some of the states are terminal, so they end the episode.\n",
    "* $r_t$ is the reward obtained at time $t$.\n",
    "* $a_t$ is the action taken by the agent at time $t$.\n",
    "* $p_s(s' | s, a)$ is the transition model, which models the probability of moving from state $s$ to state $s'$ when taking the action $a$.\n",
    "* $p_r(r | s, a)$ is the reward model, which models the probability of getting reward $r$ from state $s$ when taking the action $a$.\n",
    "\n",
    "The objective in RL is to find a policy, a probability $\\pi(a | s)$ that maximizes the expected reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving RL Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the true probabilities $p_s$  and $p_r$ are known, then the optimal policy $\\pi(a | s)$ can be calculated using dynamic programming.\n",
    "Nevertheless, usually these distributions are unknown, hence there are two approaches for solving RL problems:\n",
    "1. The **model-based methods** try to model $p_s$ and $p_r$, and then derive $\\pi(a | s)$ from these models.\n",
    "1. The **model-free methods** try to directly optimize the policy $\\pi(a | s)$.\n",
    "\n",
    "A couple of model-free methods are briefly described next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Gradient Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method directly optimizes the expected reward with respect to the policy.\n",
    "\n",
    "Since the distributions are unknown, the expected reward has no explicit form.\n",
    "Nevertheless, Monte Carlo sampling can be used to estimate the policy gradient efficiently.\n",
    "\n",
    "In practice, several episodes (trials of the agent) are run, and the obtained rewards are used to promote the strategies that lead to good results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor-Critic Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the agent learns to map the observed state to two outputs:\n",
    "1. The policy, $\\pi(a | s)$. The part of the agent responsible for this output is called the **Actor**.\n",
    "1. The estimated rewards in the future. Specifically, the sum of all rewards it expects to receive in the future. The part of the agent responsible for this output is the **Critic**.\n",
    "\n",
    "The Critic learns its task by comparing the rewards obtained in the sampled real episodes with its predictions, and applying gradient descent.\n",
    "On the other side, the Agent uses a slight modification of the policy gradient to learn its task, so that it uses also the information provided by the Critic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CartPole v0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track.\n",
    "The pendulum starts upright, and the goal is to prevent it from falling over by increasing and reducing the cart's velocity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation (State)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Index | Observation | Min | Max|\n",
    "|:-:|:-:|:-:|:-:|\n",
    "| 0 | Cart Position | $$-2.4$$ | $$2.4$$ |\n",
    "| 1 | Cart Velocity | $$-\\infty$$ | $$\\infty$$ |\n",
    "| 2 | Pole Angle | $$\\sim -41.8^\\circ$$ | $$\\sim 41.8^\\circ$$ |\n",
    "| 3 | Pole Velocity At Tip | $$-\\infty$$ | $$\\infty$$ |\n",
    "\n",
    "For the **starting value**, all observations are assigned a uniform random value between $\\pm 0.05$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions and Reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Index | Action |\n",
    "|:-:|:-:|\n",
    "0 | Push cart to the left |\n",
    "1 | Push cart to the right |\n",
    "\n",
    "The **reward** is $1$ for every step taken, including the termination step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Episode Termination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The episode finishes when any of the following conditions is met:\n",
    "1. Pole Angle is more than $\\pm 12^\\circ$.\n",
    "2. Cart Position is more than $\\pm 2.4$ (center of the cart reaches the edge of the display).\n",
    "3. Episode length is greater than $500$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell creates the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code generates an episode by taking a random policy (the next action is selected arbitrarily)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_sequence(env, model=None, early_stop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor-Critic Method with Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell configures the model.\n",
    "The parameter $\\gamma$ (variable `gamma`) determines the forgetting or discount factor for past rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "max_steps_per_episode = 10000\n",
    "env.seed(seed)\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell builds the neural network that will implement both the Actor and the Critic, sharing the input and hidden layers, and using a different output layer for the policy (Actor) and the estimation of the rewards (Critic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "num_hidden = 128\n",
    "\n",
    "inputs = keras.layers.Input(shape=(num_inputs,))\n",
    "common = keras.layers.Dense(num_hidden, activation=\"relu\")(inputs)\n",
    "action = keras.layers.Dense(num_actions, activation=\"softmax\")(common)\n",
    "critic = keras.layers.Dense(1)(common)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=[action, critic])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* What is the activation function of each output layer? Why?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training consists in completing episodes, accumulating the loss both of the actor and the critic at every step.\n",
    "During each episode, the current policy is applied.\n",
    "Once the episode is finished, the weights are updated using the gradient corresponding to both losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "huber_loss = keras.losses.Huber()\n",
    "action_probs_history = []\n",
    "critic_value_history = []\n",
    "rewards_history = []\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "\n",
    "while True:\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        for timestep in range(1, max_steps_per_episode):\n",
    "            # Show the attemps.\n",
    "            env.render()\n",
    "\n",
    "            # Estimate the policy (prediction of the next actions) and the future rewards using the model.\n",
    "            state = tf.convert_to_tensor(state)\n",
    "            state = tf.expand_dims(state, 0)\n",
    "            action_probs, critic_value = model(state)\n",
    "            critic_value_history.append(critic_value[0, 0])\n",
    "\n",
    "            # Choose random action using the policy.\n",
    "            action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n",
    "            action_probs_history.append(tf.math.log(action_probs[0, action]))\n",
    "\n",
    "            # Apply the sampled action.\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            rewards_history.append(reward)\n",
    "            episode_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Once the episode is finished, update running reward to check condition for solving.\n",
    "        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
    "\n",
    "        # Calculate the real expected value from rewards.\n",
    "        returns = []\n",
    "        discounted_sum = 0\n",
    "        for r in rewards_history[:-1]:\n",
    "            discounted_sum = r + gamma * discounted_sum\n",
    "            returns.insert(0, discounted_sum)\n",
    "        returns = np.array(returns)\n",
    "        returns = (returns - np.mean(returns)) / (np.std(returns) + eps)\n",
    "        returns = returns.tolist()\n",
    "\n",
    "        # Compute the loss values (both for Actor and Critic) to update the network.\n",
    "        history = zip(action_probs_history, critic_value_history, returns)\n",
    "        actor_losses = []\n",
    "        critic_losses = []\n",
    "        for log_prob, value, ret in history:\n",
    "            diff = ret - value\n",
    "            actor_losses.append(-log_prob * diff)\n",
    "            critic_losses.append(huber_loss(tf.expand_dims(value, 0), tf.expand_dims(ret, 0)))\n",
    "\n",
    "        # Update the weights through backpropagation.\n",
    "        loss_value = sum(actor_losses) + sum(critic_losses)\n",
    "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        # Clear variables.\n",
    "        action_probs_history.clear()\n",
    "        critic_value_history.clear()\n",
    "        rewards_history.clear()\n",
    "\n",
    "    episode_count += 1\n",
    "    if episode_count % 10 == 0:\n",
    "        print(\"Running reward: %6.2f at episode %3d\" % (running_reward, episode_count))\n",
    "\n",
    "    if running_reward > 80:\n",
    "        print(\"Solved at episode {}!\".format(episode_count))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trained model is able to generate much larger sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_sequence(env, model=model)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "197px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
