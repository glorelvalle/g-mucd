{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-weight: bold; color:#5D8AA8\" align=\"center\">\n",
    "    <div style=\"font-size: xx-large\">Métodos Avanzados en Aprendizaje Automático</div><br>\n",
    "    <div style=\"font-size: x-large; color:gray\">Review of Linear Models</div><br>\n",
    "    <div style=\"font-size: large\">Carlos María Alaíz Gudín - Universidad Autónoma de Madrid</div><br></div><hr>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initial Configuration**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell defines the configuration of Jupyter Notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    .qst {background-color: #b1cee3; padding:10px; border-radius: 5px; border: solid 2px #5D8AA8;}\n",
       "    .qst:before {font-weight: bold; content:\"Exercise\"; display: block; margin: 0px 10px 10px 10px;}\n",
       "    h1, h2, h3 {color: #5D8AA8;}\n",
       "    .text_cell_render p {text-align: justify; text-justify: inter-word;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "    .qst {background-color: #b1cee3; padding:10px; border-radius: 5px; border: solid 2px #5D8AA8;}\n",
    "    .qst:before {font-weight: bold; content:\"Exercise\"; display: block; margin: 0px 10px 10px 10px;}\n",
    "    h1, h2, h3 {color: #5D8AA8;}\n",
    "    .text_cell_render p {text-align: justify; text-justify: inter-word;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell imports the packages to be used (all of them quite standard except for `Utils`, which is provided with the notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, Lasso, lars_path, ElasticNet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from Utils import plot_dataset, plot_linear_model, evaluate_linear_model\n",
    "from Utils import plot_dataset_clas, plot_linear_model_clas, fun_cross_entropy, grad_cross_entropy\n",
    "from Utils import generate_bv_example\n",
    "from Utils import fit_polylinear_regression, pred_polylinear_regression, plot_polylinear_model\n",
    "from Utils import plot_contour_lp, plot_contour_l1_l2, plot_contour_linear_lp\n",
    "\n",
    "\n",
    "matplotlib.rc(\"figure\", figsize=(15, 5))\n",
    "seed = 123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, $\\mathcal{X} = \\mathbb{R}^d$ and $\\mathcal{Y} = \\mathbb{R}$. The models are definded by the normal vector of the hyperplane and the intercept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code generates and plots the dataset.\n",
    "As it can be seen, the underlying (real) model behind the data is linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pat = 50\n",
    "n_dim = 3\n",
    "noise = 0.5\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "wm_r = np.random.randn(n_dim)\n",
    "bm_r = np.random.randn()\n",
    "\n",
    "xm = np.random.randn(n_pat, n_dim)\n",
    "ym_r = xm @ wm_r + bm_r + noise * np.random.randn(n_pat)\n",
    "\n",
    "plot_dataset(xm, ym_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Can the linearity be distinguished also in the plot?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-Dimensional Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first approach to a multidimensional problem is to reduce it to a 1-dimensional problem by retaining only one of the features.\n",
    "The code below trains one model per feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(xm.shape[1]):\n",
    "    xmi = xm[:,i].reshape(-1, 1)\n",
    "    # Train a linear model over over feature i.\n",
    "    model = LinearRegression(fit_intercept=True)\n",
    "    model.fit(xmi, ym_r)\n",
    "    w = model.coef_[0]\n",
    "    b = model.intercept_\n",
    "\n",
    "    plot_linear_model(xmi, ym_r, w, b)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Are the previous models optimal?\n",
    "* Is this a good approach?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function computes the linear model, returning the value of the normal vector and the intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_linear_regression(X, y):\n",
    "    # Build the extended data matrix.\n",
    "    X_bar = np.column_stack((np.ones(X.shape[0]), X))\n",
    "    # Compute the optimum w and b by multiplying the pseudo-inverse times y.\n",
    "    w_bar = np.linalg.inv(X_bar.T @ X_bar) @ (X_bar.T @ y)\n",
    "    # Extract w and b from the vector of extended weights.\n",
    "    w = w_bar[1:]\n",
    "    b = w_bar[0]\n",
    "\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Look carefully at the implementation. Does it coincide with the equation shown in the slides?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell compares the true model with the estimated one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wm_e, bm_e = multiple_linear_regression(xm, ym_r)\n",
    "plot_linear_model(xm, ym_r, wm_e, bm_e, w_r=wm_r, b_r=bm_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell trains a linear model using `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model using LinearRegression.\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "# Train the model using fit.\n",
    "model.fit(xm, ym_r)\n",
    "# Extract the model parameters.\n",
    "w = model.coef_\n",
    "b = model.intercept_\n",
    "\n",
    "plot_linear_model(xm, ym_r, w, b, w_r=wm_r, b_r=bm_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Is this model equivalent to the one computed using the `multiple_linear_regression` function?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Linear Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, $\\mathcal{X} = \\mathbb{R}^2$ and $\\mathcal{Y} = \\{-1, 1\\}$ (this encoding makes simpler the prediction, only by taking the sign).\n",
    "The models are definded by the normal vector of the hyperplane and the intercept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code generates and plots the dataset.\n",
    "The underlying (real) model behind the data is linear, in the sense that the best separation is provided by a linear border."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pat = 50\n",
    "noise = 0.5\n",
    "sep = 3\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "X1 = np.random.randn(n_pat, 2)\n",
    "X2 = np.random.randn(n_pat, 2)\n",
    "sep_dir = np.random.randn(2)\n",
    "X = np.vstack((X1, X2 + sep * sep_dir / np.linalg.norm(sep_dir)))\n",
    "y = np.append(- np.ones(n_pat), np.ones(n_pat))\n",
    "\n",
    "plot_dataset_clas(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Can this be distinguished also in the plot?\n",
    "* Can this problem be solved linearly?\n",
    "* What is the influence of the `sep` variable?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first linear model can be fitted by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# The parameters of the linear model can be modified here.\n",
    "# For simplicity, the model is defined by the angle (in degrees) and the intercept.\n",
    "deg = 0\n",
    "b = 0\n",
    "########################################\n",
    "\n",
    "angle = deg / 360.0 * 2 * np.pi\n",
    "plot_linear_model_clas(X, y, [np.sin(angle), np.cos(angle)], b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* What is the best model (e.g. looking at the accuracy) that can be obtained?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality of the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells change the parameters of the model over a grid and compare the resulting models using the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = 31\n",
    "deg_vec = np.linspace(0, 360, n_points)\n",
    "ang_vec = deg_vec / 360.0 * 2 * np.pi\n",
    "b_vec = np.linspace(-2, 2, n_points)\n",
    "\n",
    "deg_mat, b_mat = np.meshgrid(deg_vec, b_vec, indexing=\"ij\")\n",
    "accs = np.zeros(deg_mat.shape)\n",
    "\n",
    "for i_ang in range(len(ang_vec)):\n",
    "    ang = ang_vec[i_ang]\n",
    "    for i_b in range(len(b_vec)):\n",
    "        b = b_vec[i_b]\n",
    "        y_p = np.sign(X @ np.array([np.sin(ang), np.cos(ang)]) + b)\n",
    "        accs[i_ang, i_b] = accuracy_score(y, y_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"Acc\"\n",
    "ind = np.unravel_index(np.argmax(accs), accs.shape)\n",
    "plt.pcolormesh(deg_mat, b_mat, accs, shading=\"gouraud\")\n",
    "\n",
    "deg_opt = deg_vec[ind[0]]\n",
    "b_opt = b_vec[ind[1]]\n",
    "\n",
    "plt.plot(deg_opt, b_opt, \"r*\")\n",
    "plt.xlabel(\"$\\\\alpha$\")\n",
    "plt.ylabel(\"$b$\")\n",
    "plt.colorbar().ax.set_ylabel(label)\n",
    "plt.show()\n",
    "print(\"Maximum %s:\\t%6.2f\" % (label, accs.max()))\n",
    "print(\"Optimal degree:\\t%6.2f\" % (deg_opt))\n",
    "print(\"Optimal b:\\t%6.2f\" % (b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* How many models have been evaluated?\n",
    "* Is this efficient, or even scalable?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell shows the dependence of the accuracy with respect to $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ang_opt = ang_vec[ind[0]]\n",
    "\n",
    "n_points = 1000\n",
    "b_vec = np.linspace(-5, 5, n_points)\n",
    "\n",
    "accs = np.zeros(n_points)\n",
    "for i_b in range(len(b_vec)):\n",
    "    b = b_vec[i_b]\n",
    "    y_p = np.sign(X @ np.array([np.sin(ang_opt), np.cos(ang_opt)]) + b)\n",
    "    accs[i_b] = accuracy_score(y, y_p)\n",
    "\n",
    "plt.plot(b_vec, accs, \"-\")\n",
    "plt.xlabel(\"$b$\")\n",
    "plt.ylabel(\"Acc\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* What is the problem of this measure?\n",
    "* Is it continuous, or there are \"jumps\"?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Regression Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first approach to linear classifiers is to train a regression linear model over the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(fit_intercept=True)\n",
    "model.fit(X, y)\n",
    "plot_linear_model_clas(X, y, model.coef_, model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Is this model optimal?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Asymmetric Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem of this approach can be clearly seen with an asymmetric dataset, where one class extends over a larger region than the other. The cell below generates such a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pat = 50\n",
    "noise = 0.5\n",
    "scale = 15\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "X1 = np.random.randn(n_pat, 2)\n",
    "X2 = np.random.randn(n_pat, 2) * [scale, 1]\n",
    "sep_dir = [1.5 * scale, 0]\n",
    "X_a = np.vstack((X1, X2 + sep_dir))\n",
    "y_a = np.append(- np.ones(n_pat), np.ones(n_pat))\n",
    "\n",
    "plot_dataset_clas(X_a, y_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell trains a linear regression model over the asymmetric dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(fit_intercept=True)\n",
    "model.fit(X_a, y_a)\n",
    "plot_linear_model_clas(X_a, y_a, model.coef_, model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Is this model optimal?\n",
    "* Can a better model be found?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple gradient descent can be used to maximize the likelihood and, hence, find the optimum parameters.\n",
    "\n",
    "In order to do so, it uses the `grad_cross_entropy` function, defined in the module `Utils`.\n",
    "This function first converts the labels to $\\mathcal{Y} = \\{0, 1\\}$, and then it computes the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X, y, max_iter=1000, eta=1e-2, evolution=False):\n",
    "    X_bar = np.column_stack((np.ones(X.shape[0]), X))\n",
    "    w_bar = np.zeros(X_bar.shape[1])\n",
    "\n",
    "    if evolution:\n",
    "        evo = np.zeros(max_iter)\n",
    "    for i in range(max_iter):\n",
    "        w_bar = w_bar - eta * grad_cross_entropy(X_bar, y, w_bar)\n",
    "        if evolution:\n",
    "            evo[i] = fun_cross_entropy(X_bar, y, w_bar)\n",
    "\n",
    "    w = w_bar[1:]\n",
    "    b = w_bar[0]\n",
    "    \n",
    "    if evolution:\n",
    "        return w, b, evo\n",
    "    else:\n",
    "        return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Does the previous implementation match the equation shown in the slides?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells below estimate the logistic regression models for the previous datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b, evolution = logistic_regression(X, y, evolution=True)\n",
    "plot_linear_model_clas(X, y, w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b = logistic_regression(X_a, y_a)\n",
    "plot_linear_model_clas(X_a, y_a, w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Is this approach better than the regression models above? Why?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evolution of the gradient descent method can also be plotted, to see how this algorithm converges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(evolution)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Objective Function\")\n",
    "plt.title(\"Convergence of the Gradient Descent\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `scikit-learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear logistic regression model can also be trained using `scikit-learn`, in particular creating a model with `LogisticRegression` and training it with the `fit` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(fit_intercept=True, solver=\"lbfgs\")\n",
    "model.fit(X, y)\n",
    "plot_linear_model_clas(X, y, model.coef_[0], model.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(fit_intercept=True, solver=\"lbfgs\")\n",
    "model.fit(X_a, y_a)\n",
    "plot_linear_model_clas(X_a, y_a, model.coef_[0], model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Are these model equivalent to the ones computed using the `logistic_regression` function?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias–Variance Illustration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell generates an illustration of the bias–variance concepts.\n",
    "Models of different complexity are trained several times over difference samples of the same problem, and then they are used to predict over the same test set.\n",
    "This allows to see the difference between the average of the predictions of a certain model and the real target (*bias*) and the variability of such predictions (*variance*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists, preds, real = generate_bv_example()\n",
    "\n",
    "plt.boxplot(preds[:, :, 0].T, showfliers=False)\n",
    "plt.plot([1, len(preds)], [real[0]] * 2, \":k\", label=\"Target\")\n",
    "\n",
    "plt.title(\"Illustration of Bias-Variance\")\n",
    "labels = [\"Model %02d\" % i for i in range(1, len(preds) + 1)]\n",
    "plt.xticks(range(1, len(preds) + 1), labels, rotation=45)\n",
    "plt.yticks([])\n",
    "plt.ylabel(\"Prediction\")\n",
    "plt.legend()\n",
    "plt.axis(\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Which models have more variance? Why?\n",
    "* Which models have more bias? Why?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Need of Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, a 1-dimensional problem is solved by generating polynomial features.\n",
    "In particular, instead of predicing $y$ using only $x$, it will be predicted using $x$, $x^2$, $x^3$...\n",
    "The number of such features is obviously related with the complexity of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is a deterministic function $y = x^2$ plus some noise. The small number of patterns can lead to over-fitting easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pat = 10\n",
    "noise = 0.1\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "x = np.linspace(-2, 2, n_pat)\n",
    "y = np.square(x) + noise * np.random.randn(n_pat)\n",
    "\n",
    "plot_dataset(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation with Polynomial Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell trains a linear model with the polynomial features $(x^1, x^2, \\dotsc, x^d)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# The degree of the model can be modified here.\n",
    "degree = 1\n",
    "########################################\n",
    "\n",
    "model = fit_polylinear_regression(x, y, degree)\n",
    "plot_polylinear_model(x, y, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* What happens when `degree = 1`?\n",
    "* And when `degree = 10`?\n",
    "* What is the optimal value of `degree`? Is it coherent with the way the dataset was generated?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A (discrete) regularization path can be built for the different values of the degree, and the train and test errors can be compared.\n",
    "In order to do so, a large test set is generated to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr = x\n",
    "y_tr = y\n",
    "x_te = np.linspace(-2, 2, 10 * n_pat)\n",
    "y_te = np.square(x_te) + noise * np.random.randn(10 * n_pat)\n",
    "\n",
    "degree_vec = np.arange(1, n_pat)\n",
    "\n",
    "mse_tr = np.zeros(len(degree_vec))\n",
    "mse_te = np.zeros(len(degree_vec))\n",
    "for i_deg in range(len(degree_vec)):\n",
    "    model = fit_polylinear_regression(x_tr, y_tr, degree_vec[i_deg])\n",
    "    mse_tr[i_deg] = mean_squared_error(y_tr, pred_polylinear_regression(model, x_tr))\n",
    "    mse_te[i_deg] = mean_squared_error(y_te, pred_polylinear_regression(model, x_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(degree_vec, mse_tr, label=\"Training\")\n",
    "plt.plot(degree_vec, mse_te, label=\"Test\")\n",
    "\n",
    "i_opt = np.argmin(mse_te)\n",
    "plt.semilogy(degree_vec[i_opt], mse_te[i_opt], \"k*\")\n",
    "\n",
    "plt.xlabel(\"Degree\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.ylim([10**-3, 10**1])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* What is the optimal value according to this regularization path?\n",
    "* Does it match the intuition?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main regularization functions are based on different $\\ell_p$ norms. In this section the level curves of the different norms can be plotted in order to help to understand their different effects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The $\\ell_p$ Norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell shows the level curves of the $\\ell_p$ norm in a $2$-dimensional context.\n",
    "This means that all the points in the same level curve have the same $\\ell_p$ norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# The value of p can be modified here.\n",
    "p = 2\n",
    "########################################\n",
    "\n",
    "plot_contour_lp(p=p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Change the value of `p` and try to appreciate the following properties:\n",
    "    - If $p = 2$, the standard distance is recovered, hence the level curves are circles.\n",
    "    - If $p \\le 1$, the $\\ell_p$ norm is non-differentiable, so corners should appear in the level curves.\n",
    "      This corners are preferred when optimizing, producing the \"sparsity\".\n",
    "    - If $p < 1$, the $\\ell_p$ norm is non-convex, so the $\\ell_1$ norm is the only convex and non-differentiable case.\n",
    "    - The $\\ell_\\infty$ norm (`p = np.inf`) corresponds to the maximum element.\n",
    "    - The $\\ell_0$ norm corresponds to the count of non-zero elements.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combination of the $\\ell_1$ Norm and the $\\ell_2$ Norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell shows the level curves of the $\\ell_1$ norm plus the $\\ell_2$ norm, according to the formula:\n",
    "$$\n",
    "\\lambda \\| x \\|_1 + (1 - \\lambda) \\| x \\|_2 ,\n",
    "$$\n",
    "where $\\lambda \\in [0, 1]$ is encoded by the variable `l1_ratio`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# The value of the mixture between L1 and L2 can be modified here (it should be between 0 and 1).\n",
    "l1_ratio = 0.5\n",
    "########################################\n",
    "\n",
    "plot_contour_l1_l2(l1_ratio=l1_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Change the value of `l1_ratio` to check the transition from the $\\ell_1$ to the $\\ell_2$.\n",
    "* Is the non-differentiable point always present? Therefore, will this regularizer produce sparsity?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Models and the $\\ell_p$ Norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get an idea of the effect of each $\\ell_p$ regularization, the following problem can be considered:\n",
    "$$ \\min_x f(\\mathbf{w}) \\quad \\text{s.t. } \\| \\mathbf{w} \\|_p \\le 1 . $$\n",
    "The function $f$ is in this case just the quadratic error of a linear model over the problem generated below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell defines the problem and plots the level curves of $f$ and the (approximate) solution of the unconstrained problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pat = 100\n",
    "noise = 0.1\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "w = np.array([1, 2])\n",
    "X = np.random.randn(n_pat, 2)\n",
    "y = X @ w + noise * np.random.randn(n_pat)\n",
    "\n",
    "plot_contour_linear_lp(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell plots the level curves of $f$, the $\\ell_p$-ball constraint and the (approximate) solution of the constrained problem.\n",
    "This solution is given by the point where the smaller level curve (the more bluish) touches the $\\ell_p$ ball.\n",
    "It should be noticed that those solutions over the axis are sparse, since one of the components is zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# The value of p can be modified here.\n",
    "p = 2\n",
    "########################################\n",
    "\n",
    "plot_contour_linear_lp(X, y, p=p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Do sparse solutions appear more frequently for $p \\le 1$? Is this related with the presence of corners?\n",
    "* What happens when $p > 1$? Is there any sparsity?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell generates a dataset with many uninformative features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(random_state=seed, noise=5e0)\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=1.0/3.0, random_state=seed)\n",
    "\n",
    "alpha_vec = np.logspace(-3, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An \"empirical\" regularization path can be estimated by changing the regularization parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_tr = np.zeros(len(alpha_vec))\n",
    "mse_te = np.zeros(len(alpha_vec))\n",
    "coefs = np.zeros((len(alpha_vec), X_tr.shape[1]))\n",
    "for i_alp in range(len(alpha_vec)):\n",
    "    model = Ridge(alpha_vec[i_alp])\n",
    "    model.fit(X_tr, y_tr)\n",
    "    mse_tr[i_alp] = mean_squared_error(y_tr, model.predict(X_tr))\n",
    "    mse_te[i_alp] = mean_squared_error(y_te, model.predict(X_te))\n",
    "    coefs[i_alp, :] = model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogx(alpha_vec, mse_tr, label=\"Training\")\n",
    "plt.semilogx(alpha_vec, mse_te, label=\"Test\")\n",
    "\n",
    "i_opt = np.argmin(mse_te)\n",
    "plt.semilogx(alpha_vec[i_opt], mse_te[i_opt], \"k*\")\n",
    "\n",
    "plt.xlabel(\"$\\\\alpha$\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evolution of the coefficients illustrates the effect of the regularizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogx(alpha_vec, coefs)\n",
    "\n",
    "plt.title(\"Ridge Regression Regularization Path\")\n",
    "plt.xlabel(\"$\\\\alpha$\")\n",
    "plt.ylabel(\"Weight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Is this evolution intuitive?\n",
    "* Does it match the theory?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An automatic search of the parameters can be done using the `GridSearchCV` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_rr = GridSearchCV(Ridge(), cv=5, param_grid={\"alpha\": alpha_vec})\n",
    "gs_rr.fit(X_tr, y_tr)\n",
    "print(\"Best Parameter: %.2g\" % gs_rr.best_params_[\"alpha\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Is the optimal value similar to the one obtained above?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same procedure can be repeated for the Lasso model, although there are specific ways of computing efficiently its regularization path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_tr = np.zeros(len(alpha_vec))\n",
    "mse_te = np.zeros(len(alpha_vec))\n",
    "coefs = np.zeros((len(alpha_vec), X_tr.shape[1]))\n",
    "for i_alp in range(len(alpha_vec)):\n",
    "    model = Lasso(alpha_vec[i_alp], max_iter=5000)\n",
    "    model.fit(X_tr, y_tr)\n",
    "    mse_tr[i_alp] = mean_squared_error(y_tr, model.predict(X_tr))\n",
    "    mse_te[i_alp] = mean_squared_error(y_te, model.predict(X_te))\n",
    "    coefs[i_alp, :] = model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogx(alpha_vec, mse_tr, label=\"Training\")\n",
    "plt.semilogx(alpha_vec, mse_te, label=\"Test\")\n",
    "\n",
    "i_opt = np.argmin(mse_te)\n",
    "plt.semilogx(alpha_vec[i_opt], mse_te[i_opt], \"k*\")\n",
    "\n",
    "plt.xlabel(\"$\\\\alpha$\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogx(alpha_vec, coefs)\n",
    "\n",
    "plt.title(\"Lasso Regularization Path\")\n",
    "plt.xlabel(\"$\\\\alpha$\")\n",
    "plt.ylabel(\"Weight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell computes the \"theoretical\" regularization path of Lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, coefs = lars_path(X_tr, y_tr, method=\"lasso\")\n",
    "\n",
    "xx = np.sum(np.abs(coefs.T), axis=1)\n",
    "xx /= xx[-1]\n",
    "\n",
    "plt.plot(xx, coefs.T)\n",
    "ymin, ymax = plt.ylim()\n",
    "plt.vlines(xx, ymin, ymax, linestyle=\"dashed\")\n",
    "\n",
    "plt.title(\"Lasso Regularization Path\")\n",
    "plt.xlabel(\"$\\\\frac{\\\\|w\\\\|_1}{\\\\max_j\\\\|w_j\\\\|}$\")\n",
    "plt.ylabel(\"Weight\")\n",
    "plt.axis(\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An automatic search of the parameters can be done using the `GridSearchCV` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_la = GridSearchCV(Lasso(max_iter=5000), cv=5, param_grid={\"alpha\": alpha_vec})\n",
    "gs_la.fit(X_tr, y_tr)\n",
    "print(\"Best Parameter: %.2g\" % gs_la.best_params_[\"alpha\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The selected features can be inspected in the optimum model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gs_la.best_estimator_.sparse_coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Is there any sparsity?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Elastic-Net, two hyper-parameters have to be set, and hence plotting a regularization path is not trivial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=Warning)\n",
    "\n",
    "gs_en = GridSearchCV(ElasticNet(max_iter=5000), cv=5,\n",
    "                  param_grid={\"alpha\": alpha_vec, \"l1_ratio\": [0, 0.05, 0.5, 0.95, 1.0]})\n",
    "gs_en.fit(X_tr, y_tr)\n",
    "print(\"Best Regularization Parameter: %.2g\" % gs_en.best_params_[\"alpha\"])\n",
    "print(\"Best Mixture Parameter: %.2g\" % gs_en.best_params_[\"l1_ratio\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the three models can be compared with the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ridge Regression: %.4f\" % gs_rr.score(X_te, y_te))\n",
    "print(\"Lasso:            %.4f\" % gs_la.score(X_te, y_te))\n",
    "print(\"Elastic-Net:      %.4f\" % gs_en.score(X_te, y_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Which model is better?\n",
    "* Is in this case Elastic-Net different from Lasso? Why?\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
