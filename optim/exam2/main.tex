\documentclass[12pt]{scrartcl}
\title{Take home exam II}
\nonstopmode
%\usepackage[utf-8]{inputenc}
\usepackage{array}
\usepackage{tabularx}
\usepackage{graphicx} % Required for including pictures
\usepackage[figurename=Figure]{caption}
\usepackage{float}    % For tables and other floats
\usepackage{verbatim} % For comments and other
\usepackage{amsmath}  % For math
\usepackage{amssymb}  % For more math
\usepackage{fullpage} % Set margins and place page numbers at bottom center
\usepackage{paralist} % paragraph spacing
\usepackage{listings} % For source code
\usepackage{subfig}   % For subfigures
%\usepackage{physics}  % for simplified dv, and 
\usepackage{enumitem} % useful for itemization
\usepackage{siunitx}  % standardization of si units
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{multicol}
\usepackage{multirow} % para las tablas
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{tikz}
\usepackage{color, colortbl}
\usepackage[margin=0.8in]{geometry} % for PAPER & MARGIN
\usepackage[many]{tcolorbox}    	% for COLORED BOXES (tikz and xcolor included)
\usepackage{setspace}               % for LINE SPACING
\usepackage{multicol}               % for MULTICOLUMNS
\setlength{\parindent}{0pt}

%\setlength\columnsep{0.25in} % setting length of column separator
\definecolor{main}{HTML}{5989cf}    % setting main color to be used
\definecolor{sub}{HTML}{cde4ff}     % setting sub color to be used

\definecolor{commentgreen}{RGB}{2,112,10}
\definecolor{highlightblue}{RGB}{31,119,220}
\definecolor{eminence}{RGB}{108,48,130}
\definecolor{weborange}{RGB}{255,129,0}
\definecolor{frenchplum}{RGB}{129,20,83}
\definecolor{darkpink}{RGB}{229,4,101}
\definecolor{gray}{gray}{0.9}


\tcbset{
    sharp corners,
    colback = white,
    before skip = 0.2cm,    % add extra space before the box
    after skip = 0.5cm      % add extra space after the box
}                           % setting global options for tcolorbox

\newtcolorbox{boxF}{
    colback = sub,
    enhanced,
    boxrule = 1.5pt, 
    colframe = white, % making the base for dash line
    borderline = {1.5pt}{0pt}{main, dashed} % add "dashed" for dashed line
}

\newtcolorbox{boxK}{
    sharpish corners, % better drop shadow
    boxrule = 0pt,
    toprule = 2pt, % top rule weight
    enhanced,
    fuzzy shadow = {0pt}{-2pt}{-0.5pt}{0.5pt}{black!35} % {xshift}{yshift}{offset}{step}{options} 
}


%%% Colours used in field vectors and propagation direction
\definecolor{mycolor}{rgb}{1,0.2,0.3}
\definecolor{brightgreen}{rgb}{0.4, 1.0, 0.0}
\definecolor{britishracinggreen}{rgb}{0.0, 0.26, 0.15}
\definecolor{cadmiumgreen}{rgb}{0.0, 0.42, 0.24}
\definecolor{ceruleanblue}{rgb}{0.16, 0.32, 0.75}
\definecolor{darkelectricblue}{rgb}{0.33, 0.41, 0.47}
\definecolor{darkpowderblue}{rgb}{0.0, 0.2, 0.6}
\definecolor{darktangerine}{rgb}{1.0, 0.66, 0.07}
\definecolor{emerald}{rgb}{0.31, 0.78, 0.47}
\definecolor{palatinatepurple}{rgb}{0.41, 0.16, 0.38}
\definecolor{pastelviolet}{rgb}{0.8, 0.6, 0.79}


\hypersetup{%
    colorlinks=True,
    urlcolor=darkpowderblue,
    citecolor=darkpowderblue,
    linkcolor=darkpowderblue
    }


\begin{document}

\begin{center}
	\hrule
	\vspace{.4cm}
	{\textbf { \large \textbf{Take home exam} \\ Part II \\ \vspace{1em} \small \textit{Convex Unconstrained and Constrained Optimization}} \\ \vspace{0.5em}\today}
\end{center}

\begin{center}
{ \vspace{0.5em} Gloria del Valle Cano \hspace{\fill}   \\}
{ gloria.valle@estudiante.uam.es \hspace{\fill} \\ \vspace{1.5em}}
	\hrule
\end{center}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{boxF}
\paragraph*{Problem 1.}(1 point) \hspace{0.15em} We have worked out the elementary version of Lagrange multipliers assuming that from $g(x, y) = 0$ we can find a function $y = h(x)$ such that $g(x, h(x)) = 0$. 

But sometimes what we get is that there is an $h$ such that $g(h(y), y) = 0$. Rewrite the Lagrange multiplier analysis in the lecture slides under this assumption.
\end{boxF}

For $f,g: \mathbb{R}^2 \to \mathbb{R}$ consider the following minimization problem

\begin{equation*}
    \min f(x, y) \mbox{ s.t. } g(x, y) = 0.
\end{equation*}

Assuming the \textbf{Implicit Function Theorem} holds, we can find a function $x = h(y)$ s.t. $g(h(y), y) = 0$ and, thus, we can write

\begin{equation*}
    f(x, y) = f(h(y), y) = \Psi(y).
\end{equation*}

At a minimum $y^\ast$ with $x^\ast = h(y^\ast)$ we thus have

\begin{equation}
    \label{eq:eq1}
    0 = \Psi ' (y^\ast) = \frac{\partial g}{\partial x}(x^\ast, y^\ast) h'(y^\ast) + \frac{\partial g}{\partial y}(x^\ast, y^\ast).
\end{equation}

But since $g(h(y), y) = 0$, we also have

\begin{equation}
    \label{eq:eq2}
    0 = \frac{\partial g}{\partial x}(x^\ast, y^\ast) h'(y^\ast) + \frac{\partial g}{\partial y}(x^\ast, y^\ast) 
    \implies h'(y^\ast) = -\frac{\frac{\partial g}{\partial y}(x^\ast, y^\ast)}{\frac{\partial g}{\partial x}(x^\ast, y^\ast)}.
\end{equation}

Putting together \ref{eq:eq1} and \ref{eq:eq2} we arrive at
\begin{equation*}
    0 = \frac{\partial f}{\partial y}(x^\ast, y^\ast) \frac{\partial g}{\partial x}(x^\ast, y^\ast) - \frac{\partial f}{\partial x}(x^\ast, y^\ast) \frac{\partial g}{\partial y}(x^\ast, y^\ast) .
\end{equation*}

That is, at $(x^\ast, y^\ast)$, $\nabla f \perp \left( - \frac{\partial g}{\partial y}, \frac{\partial g}{\partial x}\right) $ and, since 
$\left( - \frac{\partial g}{\partial y}, \frac{\partial g}{\partial x}\right) \perp \nabla g $, we have $\nabla f \| \nabla g$ i.e.
$\nabla f(x^\ast, y^\ast) = - \lambda^\ast \nabla g(x^\ast, y^\ast)$ for some $\lambda^\ast \neq 0$.

\vspace{0.5em}

Thus, for the \textbf{Lagrangian}

\begin{equation*}
    \mathcal{L}(x, y, \lambda) = f(x, y) + \lambda g(x, y),
\end{equation*}

we have a minimum $(x^\ast, y^\ast)$ there is a $\lambda^\ast \neq 0$ s.t.

\begin{equation*}
    \nabla \mathcal{L}(x^\ast, y^\ast, \lambda^\ast) = \nabla f(x^\ast, y^\ast) + \lambda^\ast \nabla g(x^\ast, y^\ast) = 0.
\end{equation*}


\vspace{0.5em}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{boxF}
\paragraph*{Problem 2.} (3 points) \hspace{0.15em} We want to solve the following constrained minimization problem: 
    \begin{align*}
        \mbox{min   \hspace{0.7em}}  & f(x, y) = x^2 + 2xy + 2y^2 - 3x + y \\
        \mbox{s.t.  \hspace{0.7em}} & x + y = 1, \\
        & x \geq 0, y \geq 0.
    \end{align*}
    
Argue first that $f$ is convex and then:

\begin{itemize}
    \item Write its Lagrangian with $\alpha, \beta$ the multipliers of the inequality constraints.
    \item Write the KKT conditions.
    \item Use them to solve the problem. For this consider separately the $(\alpha = \beta = 0), (\alpha > 0, \beta = 0), (\alpha = 0, \beta > 0), (\alpha > 0, \beta > 0)$ cases.
\end{itemize}

\end{boxF}


First of all, we verify that $f$ is convex because its Hessian matrix is positive semidefinite, or equivalently its eigenvalues are non-negative. The Hessian matrix is

\[
\mathbf{H}_f = \begin{bmatrix}\dfrac{\partial^2 f}{\partial x^2} & \dfrac{\partial^2 f}{\partial x \partial y} \\[1em]
    \dfrac{\partial^2 f}{\partial y \partial x} & \dfrac{\partial^2 f}{\partial y^2}\end{bmatrix} = \begin{bmatrix}
        2 & 2 \\ 2 & 4
    \end{bmatrix},
\]
whose eigenvalues are $\lambda_1 = 3 + \sqrt{5}$ and $\lambda_2 = 3 - \sqrt{5}$, both positive. As a result, we determine $f$ is convex.\\

Now we write its Lagrangian, with $\alpha$ and $\beta$ as multipliers of the inequality constraints and $\lambda$ as equality constraint multiplier. 


\begin{equation*}
    \mathcal{L}(x, y, \lambda, \alpha, \beta) = x^2 + 2xy + 2y^2 -3x + y + \lambda(x + y -1) - \alpha x - \beta y.
\end{equation*}

\vspace{0.5em}

Assuming that the hypothesis of the KKT conditions theorem hold, the resulting KKT conditions on a local minimum $(x^\ast, y^\ast)$ are the following:
\begin{align*}
    0 &= \frac{\partial \mathcal{L}}{\partial x} (x^\ast, y^\ast, \lambda, \alpha, \beta) \implies \lambda + 2x^\ast + 2y^\ast -3 - \alpha = 0,\\
    0 &= \frac{\partial \mathcal{L}}{\partial y} (x^\ast, y^\ast, \lambda, \alpha, \beta) \implies \lambda + 2x^\ast + 4y^\ast + 1 - \beta = 0,\\
    0 &= \alpha x^\ast ,\\
    0 &= \beta y^\ast .\\
\end{align*}
Then, we use them to solve the problem, considering the four possible cases below:
\begin{itemize}
    \item Case $\alpha = \beta = 0$.
    \begin{align*}
        0 &= \frac{\partial \mathcal{L}}{\partial x} = \lambda + 2x + 2y -3 \\
        0 &= \frac{\partial \mathcal{L}}{\partial y} = \lambda + 2x + 4y + 1 \\
    \end{align*}
    From both expressions we get $2x + 2y -3 = 2x + 4y +1 \implies 2y = -4 \implies y = -2$. Then, $x = 1-y = 3$ and $\lambda=-1$. So we have $\mathbf{(3, -2)'}$ as \textbf{feasible KKT point}.
    \item Case $\alpha > 0, \beta = 0$.
    When $\alpha > 0$, $x=0$, so $y=1$ and $\lambda=-5$. Therefore we have a \textbf{feasible KKT point} on $\mathbf{(0, 1)'}$.
    \item Case $\alpha = 0, \beta > 0$.
    When $\beta > 0$, $y=0$, so $x=1$ and $\lambda=1$. Therefore we have a \textbf{feasible KKT point} on $\mathbf{(1, 0)'}$.
    \item Case $\alpha > 0, \beta > 0$.
    This implies $x=y=0$, so we have a contradiction because $x + y \neq 1$. This means that $(0,0)'$ \textbf{is not} a feasible KKT point.
\end{itemize}

Given all points found, we can determine that our optimal solution is $\mathbf{(1, 0)'}$, with an optimal value $\mathbf{\lambda=1}$.

\vspace{0.5em}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{boxF}
\paragraph*{Problem 3.} (1 point) \hspace{0.15em} Let $f : S \subset \mathbb{R}^d \to \mathbb{R}$ be a convex function on the convex set $S$ and we extend it to an $\tilde f : \mathbb{R}^d \to \mathbb{R}$ as:

    \begin{align*}
        \tilde f(x) &= f(x) \mbox{ if } x \in S. \\
                    &= +\infty \mbox{ if } x \notin S.
    \end{align*}

Show that $\tilde f$ is a convex function on $\mathbb{R}^d$. Assume that $a + \infty = \infty $ and that $ a \cdot \infty = \infty$ for $a > 0$.
\end{boxF}

We say that $S$ is a \textbf{convex set} if for all $x, x' \in S$ and $\lambda \in [0,1]$,

\begin{equation*}
    \lambda x + (1 - \lambda) x' \in S.
\end{equation*}

Let $x, x' \in S$ and $\lambda \in [0,1]$, so here we cover two cases:

\begin{itemize}
    \item \textit{First case}. If $x, x' \in S$:
    \begin{equation*}
        \tilde f (\lambda x + (1- \lambda)x') = f (\lambda x + (1- \lambda)x')  \leq \lambda f(x) + (1-\lambda)f(x') = \lambda \tilde f(x) + (1-\lambda)\tilde f(x').
    \end{equation*}
    Where the first equality holds given that $S$ is convex. The inequality holds because $f$ is convex. And, the last equality raises from the definition of $\tilde f$.

    \item \textit{Second case}. If $x \notin S$ or $x' \notin S$, we have that
    \begin{equation*}
        \tilde f (\lambda x + (1- \lambda)x') \leq \lambda \tilde f (x) + (1 - \lambda) \tilde f (x') = + \infty,
    \end{equation*}
    because $\tilde f (y) \leq + \infty$, $\forall y \in \mathbb{R}^d$.
\end{itemize}

Since both cases satisfy convexity definition holds, we conclude this function is convex.

\vspace{0.5em}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{boxF}
\paragraph*{Problem 4.} (2 points) \hspace{0.15em} Prove \textbf{Jensen's inequality}: if $f$ is convex on $\mathbb{R}^d$ and $\sum_1^k \lambda_i = 1$, with $0 \leq \lambda_i \leq 1$ we have for any $x_1, \dots, x_k \in \mathbb{R}^d$

    $$
    f\left(\sum_1^k \lambda_i x_i\right) \leq \sum_1^k \lambda_i f(x_i)
    $$

\textit{Hint: just write $\sum_1^k \lambda_i x_i = \lambda_1 x_1 + \left( 1 - \lambda_1\right)$ for an appropriate $v$ and apply repeatedly the definition of a convex function. Start with $k = 3$ and carry on.}

\end{boxF}

We proceed using an inductive procedure:

\begin{itemize}
    \item If $k=1$ then $\lambda = 1$, so we simply have $f(x_1) = f(x_1)$, which is true, and
    nothing to prove. If $k=2$ we have the definition of the convexity of $f$:
    \begin{equation*}
        \lambda_1 + \lambda_2 = 1, \ \lambda_1, \lambda_2 \geq 0 \implies f(\lambda_1 x_1 + \lambda_2 x_2) \leq \lambda_1 f(x_1) + \lambda_2 f(x_2).
    \end{equation*}
    \item We assume the statement its true for $k$ and consider $k+1$ points $x_1, \dots, x_{k+1}$, with coefficients $\lambda_1, \dots, \lambda_{k+1} \geq 0$, $\sum_{i=1}^{k+1} \lambda_i = 1$. The evaluation of the linear combination can be decomposed as
    \begin{equation*}
        f\left( \sum_{i=1}^{k+1} \lambda_i  x_i \right) = f\left( (1-\lambda_1) \left(\sum_{i=2}^{k+1} \frac{\lambda_i}{1-\lambda_1} x_i\right) + \lambda_1 x_1\right).
    \end{equation*}
    Using this, it is straightforward to use the Jensen's inequality on $x_1$ and $\sum_{i=2}^{k+1} \frac{\lambda_i}{1-\lambda_1} x_i $ with coefficients $\lambda_1$ and $1-\lambda_1$  respectively.  That is,

    \begin{equation*}
        f\left( \sum_{i=1}^{k+1} \lambda_i  x_i \right) \leq (1 - \lambda_1) f \left( \sum_{i=2}^{k+1} \frac{\lambda_i}{1-\lambda_1} x_i \right) + \lambda_1 f(x_1).
    \end{equation*}
    We may notice that $\sum_{i=2}^{k+1} \frac{\lambda_i}{1-\lambda_1} x_i$ verifies the inductive hypothesis, thus,
    \begin{equation*}
        f \left( \sum_{i=2}^{k+1} \frac{\lambda_i}{1-\lambda_1} x_i \right) \leq \sum_{i=2}^{k+1} \frac{\lambda_i}{1-\lambda_1} f(x_i).
    \end{equation*}
    Finally,
    \begin{equation*}
        f\left(\sum_{i=1}^{k+1} \lambda_i x_i \right) \leq (1-\lambda_1) \sum_{i = 2}^{k+1} \frac{\lambda_i}{1-\lambda_1}f(x_i) + \lambda_1 f(x_1) = \sum_{i=1}^{k+1} \lambda_i f(x_i).
    \end{equation*}
\end{itemize}


\vspace{0.5em}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{boxF}
\paragraph*{Problem 5.} (3 points) \hspace{0.15em} Prove that the following function is convex

    \begin{align*}
        f(x) &= x^2 -1, \hspace{3.3em} |x| > 1 \\
             &= 0       \hspace{6em} |x| \leq 1
    \end{align*}

and compute its proximal. Which are the fixed points of this proximal?
\end{boxF}

We note that $f$ can be seen as the maximum of two functions $f(x) = \max\{0, x^2-1\}$. Both of these functions are convex. Then, we are going to show that the maximum of two convex functions is also convex. \\

Let $m$ and $n$ be two convex functions and $h(x) = \max\{m(x), n(x)\}$. Given $\lambda \in [0,1]$ and $x, y \in \mathbb{R}$ we aim to show that 

\[
    h(\lambda x + (1- \lambda)y) \leq \lambda h(x) + (1- \lambda)h(y).
\]

On one hand, it is clear that 

\[
    m\left(\lambda x + (1- \lambda)y \right) \leq  \lambda m(x) + (1- \lambda)m(y) \leq  \lambda h(x) + (1- \lambda)h(y),
\]

where in the first inequality comes from the fact that $m$ is convex and the second from the definition of $h$. The same inequalities holds for $n$. Given that both functions are upper bounded by the same value, the maximum is also upper bounded by this value, so $h$ is convex. Using this auxiliary result, $f$ is convex. \\

%Now we compute the proximal of $f$, by definition.

%\[
%    \mbox{prox}_f (x) = \underset{z}{\mbox{argmin}} \left\{f(z) + \frac 1 2 \| z - x\|^2\right\} = \underset{z}{\mbox{argmin}}
%    \begin{cases}
%        z^2 -1 + \frac 1 2 ( z- x)^2,  &  |z| > 1\\
%        \frac 1 2 (z -x)^2, & |z| \leq 1
%    \end{cases}.
%\]
Now we compute the proximal operator of $f$ as $\mbox{prox}_f = (I + \partial f)^{-1}$ . Note that $f$ is differentiable, so $\partial f (x) = \left\{ f'(x) \right\}$. Using this:

\[
    (I + \partial f) (x) = 
    \begin{cases}
        \{3x\} &  x \in (-\infty, -1),\\
        \{x\} & x \in  [-1, 1], \\
        \{3x\} & x \in (1, +\infty).
    \end{cases}
\]

As a result, we get the inverse operator

\[
    (I + \partial f)^{-1} (x) = 
    \begin{cases}
        \{\frac x 3 \}& x \in  (-\infty, -3], \\
        \{-1\} & x \in (-3,-1), \\
        \{x\} &  x \in [-1, 1],\\
        \{1\} & x \in (1,3), \\
        \{\frac x 3\} & x \in  [3, \infty),
    \end{cases}
\]

where the values at $(-3, -1)$ and $(1, 3)$ corresponds to the breaks of $(I + \partial f) (x)$. Finally, we ilustrate $(\text{I} + \partial f)^{-1}(x)$ for a better comprehension.\\

\begin{figure}[H]
    \centering
    \begin{tikzpicture}[scale=1.5]
        \draw[->] (-4, 0) -- (4, 0) node[right] {$x$};
        \draw[->] (0, -2) -- (0, 2) node[above] {$(\text{I} + \partial f)^{-1}(x)$};
        \draw[thick, domain=-4:-3, smooth, variable=\x, teal] plot ({\x}, {\x/3});
        \draw[thick, domain=-3:-1, smooth, variable=\y, teal] plot ({\y}, {-1});
        \draw[thick, domain=-1:1, smooth, variable=\x, teal] plot ({\x}, {\x});
        \draw[thick, domain=1:3, smooth, variable=\y, teal] plot ({\y}, {1});
        \draw[thick, domain=3:4, smooth, variable=\x, teal] plot ({\x}, {\x/3});
        
        % Dashed lines
        \path[draw=gray, dashed, thick] (-1,-1) -- (-1,0);
        \path[draw=gray, dashed, thick] (-1,-1) -- (0,-1);
        \path[draw=gray, dashed, thick] (-3,-1) -- (-3,0);
        \path[draw=gray, dashed, thick] (3,0) -- (3,1);
        \path[draw=gray, dashed, thick] (1,0) -- (1,1);
        \path[draw=gray, dashed, thick] (0,1) -- (1,1);
        
        % ticks
        \node[inner sep=1pt,] at (-1, 0.2) {$-1$};
        \node[inner sep=1pt,] at (1, -0.2) {$1$};
        \node[inner sep=1pt,] at (0.25, -1) {$-1$};
        \node[inner sep=1pt,] at (-3, 0.2) {$-3$};
        \node[inner sep=1pt,] at (-0.2, 1) {$1$};
        \node[inner sep=1pt,] at (3, -0.2) {$3$};
    \end{tikzpicture}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\end{document}