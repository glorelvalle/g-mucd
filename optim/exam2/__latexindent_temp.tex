\documentclass[12pt]{scrartcl}
\title{Take home exam II}
\nonstopmode
%\usepackage[utf-8]{inputenc}
\usepackage{array}
\usepackage{tabularx}
\usepackage{graphicx} % Required for including pictures
\usepackage[figurename=Figure]{caption}
\usepackage{float}    % For tables and other floats
\usepackage{verbatim} % For comments and other
\usepackage{amsmath}  % For math
\usepackage{amssymb}  % For more math
\usepackage{fullpage} % Set margins and place page numbers at bottom center
\usepackage{paralist} % paragraph spacing
\usepackage{listings} % For source code
\usepackage{subfig}   % For subfigures
%\usepackage{physics}  % for simplified dv, and 
\usepackage{enumitem} % useful for itemization
\usepackage{siunitx}  % standardization of si units
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{multicol}
\usepackage{multirow} % para las tablas
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{hyperref}
	
\usepackage{color, colortbl}
\usepackage[margin=0.8in]{geometry} % for PAPER & MARGIN
\usepackage[many]{tcolorbox}    	% for COLORED BOXES (tikz and xcolor included)
\usepackage{setspace}               % for LINE SPACING
\usepackage{multicol}               % for MULTICOLUMNS
\setlength{\parindent}{0pt}

%\setlength\columnsep{0.25in} % setting length of column separator
\definecolor{main}{HTML}{5989cf}    % setting main color to be used
\definecolor{sub}{HTML}{cde4ff}     % setting sub color to be used

\definecolor{commentgreen}{RGB}{2,112,10}
\definecolor{highlightblue}{RGB}{31,119,220}
\definecolor{eminence}{RGB}{108,48,130}
\definecolor{weborange}{RGB}{255,129,0}
\definecolor{frenchplum}{RGB}{129,20,83}
\definecolor{darkpink}{RGB}{229,4,101}
\definecolor{gray}{gray}{0.9}


\tcbset{
    sharp corners,
    colback = white,
    before skip = 0.2cm,    % add extra space before the box
    after skip = 0.5cm      % add extra space after the box
}                           % setting global options for tcolorbox

\newtcolorbox{boxF}{
    colback = sub,
    enhanced,
    boxrule = 1.5pt, 
    colframe = white, % making the base for dash line
    borderline = {1.5pt}{0pt}{main, dashed} % add "dashed" for dashed line
}

\newtcolorbox{boxK}{
    sharpish corners, % better drop shadow
    boxrule = 0pt,
    toprule = 2pt, % top rule weight
    enhanced,
    fuzzy shadow = {0pt}{-2pt}{-0.5pt}{0.5pt}{black!35} % {xshift}{yshift}{offset}{step}{options} 
}


%%% Colours used in field vectors and propagation direction
\definecolor{mycolor}{rgb}{1,0.2,0.3}
\definecolor{brightgreen}{rgb}{0.4, 1.0, 0.0}
\definecolor{britishracinggreen}{rgb}{0.0, 0.26, 0.15}
\definecolor{cadmiumgreen}{rgb}{0.0, 0.42, 0.24}
\definecolor{ceruleanblue}{rgb}{0.16, 0.32, 0.75}
\definecolor{darkelectricblue}{rgb}{0.33, 0.41, 0.47}
\definecolor{darkpowderblue}{rgb}{0.0, 0.2, 0.6}
\definecolor{darktangerine}{rgb}{1.0, 0.66, 0.07}
\definecolor{emerald}{rgb}{0.31, 0.78, 0.47}
\definecolor{palatinatepurple}{rgb}{0.41, 0.16, 0.38}
\definecolor{pastelviolet}{rgb}{0.8, 0.6, 0.79}


\hypersetup{%
    colorlinks=True,
    urlcolor=darkpowderblue,
    citecolor=darkpowderblue,
    linkcolor=darkpowderblue
    }


\begin{document}

\begin{center}
	\hrule
	\vspace{.4cm}
	{\textbf { \large \textbf{Take home exam} \\ Part II \\ \vspace{1em} \textit{Convex Unconstrained and Constrained Optimization}} \\ \vspace{0.5em}\today}
\end{center}

\begin{center}
{ \vspace{0.5em} Gloria del Valle Cano \hspace{\fill}   \\}
{ gloria.valle@estudiante.uam.es \hspace{\fill} \\ \vspace{1.5em}}
	\hrule
\end{center}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{boxF}
\paragraph*{Problem 1.}(1 point) \hspace{0.15em} We have worked out the elementary version of Lagrange multipliers assuming that from $g(x, y) = 0$ we can find a function $y = h(x)$ such that $g(x, h(x)) = 0$. 

But sometimes what we get is that there is an $h$ such that $g(h(y), y) = 0$. Rewrite the Lagrange multiplier analysis in the lecture slides under this assumption.
\end{boxF}

For $f,g: \mathbb{R}^2 \to \mathbb{R}$ consider the following minimization problem

\begin{equation*}
    \min f(x, y) \mbox{ s.t. } g(x, y) = 0.
\end{equation*}

Assuming the \textbf{Implicit Function Theorem} holds, we can find a function $x = h(y)$ s.t. $g(h(y), y) = 0$ and, thus, we can write

\begin{equation*}
    f(x, y) = f(h(y), y) = \Psi(y).
\end{equation*}

At a minimum $y^\ast$ with $x^\ast = h(y^\ast)$ we thus have

\begin{equation}
    \label{eq:eq1}
    0 = \Psi ' (y^\ast) = \frac{\partial g}{\partial x}(x^\ast, y^\ast) h'(y^\ast) + \frac{\partial g}{\partial y}(x^\ast, y^\ast).
\end{equation}

But since $g(h(y), y) = 0$, we also have

\begin{equation}
    \label{eq:eq2}
    0 = \frac{\partial g}{\partial x}(x^\ast, y^\ast) h'(y^\ast) + \frac{\partial g}{\partial y}(x^\ast, y^\ast) 
    \implies h'(y^\ast) = -\frac{\frac{\partial g}{\partial y}(x^\ast, y^\ast)}{\frac{\partial g}{\partial x}(x^\ast, y^\ast)}.
\end{equation}

Putting together \ref{eq:eq1} and \ref{eq:eq2} we arrive at
\begin{equation*}
    0 = \frac{\partial f}{\partial y}(x^\ast, y^\ast) \frac{\partial g}{\partial x}(x^\ast, y^\ast) - \frac{\partial f}{\partial x}(x^\ast, y^\ast) \frac{\partial g}{\partial y}(x^\ast, y^\ast) .
\end{equation*}

That is, at $(x^\ast, y^\ast)$, $\nabla f \perp \left( - \frac{\partial g}{\partial y}, \frac{\partial g}{\partial x}\right) $ and, since 
$\left( - \frac{\partial g}{\partial y}, \frac{\partial g}{\partial x}\right) \perp \nabla g $, we have $\nabla f \| \nabla g$ i.e.
$\nabla f(x^\ast, y^\ast) = - \lambda^\ast \nabla g(x^\ast, y^\ast)$ for some $\lambda^\ast \neq 0$.

\vspace{0.5em}

Thus, for the \textbf{Lagrangian}

\begin{equation*}
    \mathcal{L}(x, y, \lambda) = f(x, y) + \lambda g(x, y),
\end{equation*}

we have a minimum $(x^\ast, y^\ast)$ there is a $\lambda^\ast \neq 0$ s.t.

\begin{equation*}
    \nabla \mathcal{L}(x^\ast, y^\ast, \lambda^\ast) = \nabla f(x^\ast, y^\ast) + \lambda^\ast \nabla g(x^\ast, y^\ast) = 0.
\end{equation*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{boxF}
\paragraph*{Problem 2.} (3 points) \hspace{0.15em} We want to solve the following constrained minimization problem: 
    \begin{align*}
        \mbox{min   \hspace{0.7em}}  & f(x, y) = x^2 + 2xy + 2y^2 - 3x + y \\
        \mbox{s.t.  \hspace{0.7em}} & x + y = 1, \\
        & x \geq 0, y \geq 0.
    \end{align*}
    
Argue first that $f$ is convex and then:

\begin{itemize}
    \item Write its Lagrangian with $\alpha, \beta$ the multipliers of the inequality constraints.
    \item Write the KKT conditions.
    \item Use them to solve the problem. For this consider separately the $(\alpha = \beta = 0), (\alpha > 0, \beta = 0), (\alpha = 0, \beta > 0), (\alpha > 0, \beta > 0)$ cases.
\end{itemize}

First, we know that $f$ is nor convex because it has two convex inequality constraints, and an affine equality one. Also, there are 

\end{boxF}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{boxF}
\paragraph*{Problem 3.} (1 point) \hspace{0.15em} Let $f : S \subset \mathbb{R}^d \to \mathbb{R}$ be a convex function on the convex set $S$ and we extend it to an $\tilde f : \mathbb{R}^d \to \mathbb{R}$ as:

    \begin{align*}
        \tilde f(x) &= f(x) \mbox{ if } x \in S. \\
                    &= +\infty \mbox{ if } x \notin S.
    \end{align*}

Show that $\tilde f$ is a convex function on $\mathbb{R}^d$. Assume that $a + \infty = \infty $ and that $ a \cdot \infty = \infty$ for $a > 0$.
\end{boxF}

We say that $S$ is a \textbf{convex set} if for all $x, x' \in S$ and $\lambda \in [0,1]$,

\begin{equation*}
    \lambda x + (1 - \lambda) x' \in S.
\end{equation*}

Let $x, x' \in S$ and $\lambda \in [0,1]$, so here we cover two cases:

\begin{itemize}
    \item \textit{First case}. If $x, x' \in S$:
    \begin{equation*}
        \tilde f (\lambda x + (1- \lambda)x') = f (\lambda x + (1- \lambda)x')  \leq \lambda f(x) + (1-\lambda)f(x') = \lambda \tilde f(x) + (1-\lambda)\tilde f(x').
    \end{equation*}
    Where the first equality holds given that $S$ is convex. The inequality holds because $f$ is convex. And, the last equality raises from the definition of $\tilde f$.

    \item \textit{Second case}. If $x \notin S$ or $x' \notin S$, we have that
    \begin{equation*}
        \tilde f (\lambda x + (1- \lambda)x') \leq \lambda \tilde f (x) + (1 - \lambda) \tilde f (x') = + \infty,
    \end{equation*}
    because $\tilde f (y) \leq + \infty$, $\forall y \in \mathbb{R}^d$.
\end{itemize}

Since both cases satisfy convexity definition holds, we conclude this function is convex.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{boxF}
\paragraph*{Problem 4.} (2 points) \hspace{0.15em} Prove \textbf{Jensen's inequality}: if $f$ is convex on $\mathbb{R}^d$ and $\sum_1^k \lambda_i = 1$, with $0 \leq \lambda_i \leq 1$ we have for any $x_1, ..., x_k \in \mathbb{R}^d$

    $$
    f\left(\sum_1^k \lambda_i x_i\right) \leq \sum_1^k \lambda_i f(x_i)
    $$

\textit{Hint: just write $\sum_1^k \lambda_i x_i = \lambda_1 x_1 + \left( 1 - \lambda_1\right)$ for an appropriate $v$ and apply repeatedly the definition of a convex function. Start with $k = 3$ and carry on.}

\end{boxF}

We proceed using an inductive procedure:

\begin{itemize}
    \item If $k=1$ then $\lambda = 1$, so we simply have $f(x_1) = f(x_1)$, which is true, and
    nothing to prove. If $k=2$ we have the definition of the convexity of $f$:
    \begin{equation*}
        \lambda_1 + \lambda_2 = 1, \ \lambda_1, \lambda_2 \geq 0 \implies f(\lambda_1 x_1 + \lambda_2 x_2) \leq \lambda_1 f(x_1) + \lambda_2 f(x_2).
    \end{equation*}
    \item We assume the statement its true for $k$ and consider $k+1$ points $x_1, \dots, x_{k+1}$, with coefficients $\lambda_1, \dots, \lambda_{k+1} \geq 0$, $\sum_{i=1}^{k+1} \lambda_i = 1$. The evaluation of the linear combination can be decomposed as
    \begin{equation*}
        f\left( \sum_{i=1}^{k+1} \lambda_i  x_i \right) = f\left( (1-\lambda_1) \left(\sum_{i=2}^{k+1} \frac{\lambda_i}{1-\lambda_1} x_i\right) + \lambda_1 x_1\right).
    \end{equation*}
    Using this, it is straightforward to use the Jensen's inequality on $x_1$ and $\sum_{i=2}^{k+1} \frac{\lambda_i}{1-\lambda_1} x_i $ with coefficients $\lambda_1$ and $1-\lambda_1$  respectively.  That is,

    \begin{equation*}
        f\left( \sum_{i=1}^{k+1} \lambda_i  x_i \right) \leq (1 - \lambda_1) f \left( \sum_{i=2}^{k+1} \frac{\lambda_i}{1-\lambda_1} x_i \right) + \lambda_1 f(x_1).
    \end{equation*}
    We may notice that $\sum_{i=2}^{k+1} \frac{\lambda_i}{1-\lambda_1} x_i$ verifies the inductive hypothesis, thus,
    \begin{equation*}
        f \left( \sum_{i=2}^{k+1} \frac{\lambda_i}{1-\lambda_1} x_i \right) \leq \sum_{i=2}^{k+1} \frac{\lambda_i}{1-\lambda_1} f(x_i).
    \end{equation*}
    Finally,
    \begin{equation*}
        f\left(\sum_{i=1}^{k+1} \lambda_i x_i \right) \leq (1-\lambda_1) \sum_{i = 2}^{k+1} \frac{\lambda_i}{1-\lambda_1}f(x_i) + \lambda_1 f(x_1) = \sum_{i=1}^{k+1} \lambda_i f(x_i).
    \end{equation*}
\end{itemize}


 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{boxF}
\paragraph*{Problem 5.} (3 points) \hspace{0.15em} Prove that the following function is convex

    \begin{align*}
        f(x) &= x^2 -1, \hspace{3.3em} |x| > 1 \\
             &= 0       \hspace{6em} |x| \leq 1
    \end{align*}

and compute its proximal. Which are the fixed points of this proximal?
\end{boxF}



\vspace{1em}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\end{document}